\documentclass{article}
\title{Tutorial Solutions: Cluster Analysis}
\author{High Dimensional Data Analysis}
\date{Week 9}
\begin{document}
\maketitle
\section*{Concepts}
\begin{enumerate}
  \item Why might a researcher use cluster analysis?
  {\em
  Cluster analysis is a multivariate technique that attempts to split a large dataset into a smaller set of groups. The elements within the groups are similar (homogeneous) in their characteristics, while the characteristics between each of the groups are diverse (heterogeneous).  So a researcher might use cluster analysis for data reduction (i.e. identify major groups within a population) or to examine hypotheses about the behavior of different groups in the population, identifying market segmentation and determining target markets. Cluster analysis is also useful for product positioning and new product development, and selecting test markets.
  
  There is always a trade-off between homogeneity within clusters (achieved by having lots of clusters) and parsimony (having as few clusters as possible and less homogeneity within the cluster).   Remember, clustering is like having mini samples within the overall sample, so we need to make sure the mini-samples have enough elements in them to do some stats on!	 
  }
  \item Distinguish between hierarchical and non-hierarchical cluster techniques. In what situations would you use each of these techniques?
  {\em
  Hierarchical: are sequential either building up from separate clusters or breaking down from the one cluster.  The advantages of these methods are that they are simple since all possible solutions can be seen in one analysis.  Also, measures of similarity have been extensively developed and the researcher can quickly evaluate alternative solutions, varying measures of similarity and 	linkage methods.  Care must be taken as early combinations can persist (linkage).  To avoid this we tend to use the average linkage method, centroid method or Ward’s method.  These methods are also sensitive to outliers.
  
  
  Non-hierarchical: The clustering process is highly mechanical and the clustering algorithms used will only group similar observations to form clusters (we used the k-means algorithm – there are many other methods).  It is up to the researcher to determine the characteristics of each cluster and name them accordingly.  The results are not typically sensitive to outliers.  However, the researcher needs to have some understanding of how to create the seed points.  Moreover, validation is very important since an optimal solution is not guaranteed.  Several analyses need to be performed to find what appears as the optimum solution.
  
  Hierarchical are suited to small data sets (because of limitations on storage capacity and analysis requirements as the data set gets larger).  They are also useful when the research is purely exploratory.
  
  Non-hierarchical methods are well suited to large data sets.  Since the number of clusters are pre-determined, there is no need for dendrograms (which are difficult to read with large data sets).  They can also be used when previous research exists as the researcher can determine a range of sensible cluster solutions to investigate.	
  }
\end{enumerate}
\section*{Application: Hierarchical Clustering}
{\em See R Code for solutions}\\
This section uses the mtcars dataset which is available with R.
\begin{enumerate}
	\item Cluster the data using Ward's D2 method (with squared distances) and Euclidean distance.
	\item Produce a dendrogram for the above analysis
	\item Discuss whether the following choices for the number of clusters are suitable	or not
	\begin{enumerate}
		\item One-cluster
		\item Two-cluster
		\item Three-cluster
		\item Four-cluster
	\end{enumerate}

	\item  For the 2-cluster solution, store the cluster membership in a new variable.
	\item  Repeat part 4 using:
	\begin{enumerate}
		\item Average Linkage
		\item Centroid Method
		\item Complete Linkage Method
	\end{enumerate}

	\item Find the adjusted Rand Index between your answer to question 4 and
	\begin{enumerate}
		\item Your answer to 5 (a)
		\item Your answer to 5 (b)
		\item Your answer to 5 (c)
	\end{enumerate}
\end{enumerate}
\section*{Application: Non-Hierarchical Clustering}
{\em See R Code for solutions}\\
This section uses the Wholesale dataset which is available on Moodle and was originally obtained from the UCI machine learning repository.  The purpose of this section is to implement the following procedure
\begin{enumerate}
	\item Separate the data into a training sample with 220 observations and a test sample  with  220 observations.   The  same  observation  cannot  appear  in both samples.
	\item  Using the {\bf test} sample only
	\begin{enumerate}
		\item Run k-means clustering with two clusters
		\item  Obtain the cluster membership of each {\bf test}
		observation
	\end{enumerate}
	\item  Using the {\bf training} sample only
	\begin{enumerate}
		\item Run k-means clustering with two clusters
		\item Obtain the center of each cluster
		\item For  each {\bf test} observation  find  the nearest  center  obtained  at  step 3(b).
		\item Allocate  each  test  observation  to  the  cluster  corresponding  to  the
		nearest center.
		\item This gives the cluster membership of each test observation.
	\end{enumerate}
	\item  Compare the result in 2(b) and 3(e) using a cross tab and adjusted Rand Index
    \item  Repeat steps 2-4 with 3 clusters.
\end{enumerate}
{\bf Tips:} To split up the sample, first we need to select which observation go into the test sample and which ones go into the training sample. The function {\em sample} can be useful here.  Also, Steps 3(b)-3(e) looks quite difficult but can be done using the function {\em cl\_predict}
which is available in the R package {\em clue}.

\end{document}